# üß© Kubernetes Core Concepts - Init Containers & Taints and Tolerations

---

## üöÄ 1. Init Containers

### üìå What is an Init Container?
Init Containers are special containers that run **before the main application container starts** in a Pod. They are used to **perform initialization logic**‚Äîlike setup tasks, checks, or pre-conditions.

### üß† Think of it like:
> A chef preparing the kitchen (chopping, heating oven) **before** the cook arrives to start actual cooking.

### üîß Why Use Init Containers?
- To run scripts/tools before your main app container.
- To **wait for a dependency** (like a DB becoming ready).
- To **fetch configs** or initialize volumes.

### üîÑ Workflow:
1. Init container 1 runs and completes.
2. Init container 2 runs and completes (if present).
3. Then main container(s) start.

### ‚úÖ DevOps Use Case:
In CI/CD pipeline, your app container shouldn't start unless a config is downloaded from a secrets store. Use an Init Container to download it first.

### ‚úèÔ∏è YAML Example:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-container-demo
spec:
  containers:
  - name: main-app
    image: busybox
    command: ['sh', '-c', 'echo Main container started! && sleep 3600']

  initContainers:
  - name: init-script
    image: busybox
    command: ['sh', '-c', 'echo Init container running! && sleep 5']
```

### üîç Notes:
- Init containers **run sequentially**.
- If one fails, the Pod **won‚Äôt proceed** to the next or main container.
- They don‚Äôt support `livenessProbe` or `readinessProbe` (not needed).

### ü§ñ General Analogy:
> Before the movie starts, there are ads and a trailer‚Äîthese are like Init containers preparing the audience.

---

## üö´ 2. Taints and Tolerations

### üìå What Are Taints?
Taints are applied on **nodes** to **repel** certain Pods from scheduling on them unless the Pods explicitly tolerate them.

### üìå What Are Tolerations?
Tolerations are applied on **Pods** to allow them to be scheduled on tainted nodes.

### üß† Think of it like:
> A hotel room with a "Do Not Disturb" sign (taint). Only VIPs with a special pass (toleration) can enter.

### üîß Why Use Them?
- To **isolate workloads** (e.g., GPU nodes for ML jobs).
- To **protect system or infra nodes**.
- For **dedicated node pools**.

### üîÑ Workflow:
1. Node is tainted (e.g., `key=value:NoSchedule`).
2. Pod with matching toleration can be scheduled.
3. Other Pods will be rejected from that node.

### ‚úÖ DevOps Use Case:
You have logging agents or monitoring tools like **Datadog Agent** that should only run on certain nodes. Taint the node, and only let Pods with toleration run on it.

### ‚úèÔ∏è Taint Example:
```bash
kubectl taint nodes node1 key=value:NoSchedule
```

### ‚úèÔ∏è Toleration in Pod YAML:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: toleration-demo
spec:
  containers:
  - name: app
    image: nginx
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"
```

### ü§î Common Doubts:
- **Can a Pod run without toleration on a tainted node?**
  No. The scheduler won‚Äôt place it there.

- **Can I taint a node dynamically?**
  Yes, using `kubectl taint`.

- **What happens if no node accepts the Pod?**
  The Pod will stay in a **Pending** state.

---

## üß© Summary Table:
| Feature               | Init Container               | Taints & Tolerations            |
|-----------------------|------------------------------|---------------------------------|
| Purpose               | Setup tasks before main app | Control where pods run          |
| Level                 | Pod level                    | Node and Pod level              |
| Order                 | Runs before app container    | Runtime scheduling restriction  |
| Failure               | Fails pod creation           | Prevents scheduling             |
| DevOps Usage          | Fetch configs, wait for DB   | Restrict logging/infra pods     |

---

‚úÖ Let me know if you'd like to see:
- Node Affinity (vs Tolerations)
- InitContainer real-time use cases (Vault, DNS checks)
- More YAML exercises

# Node Affinity and Anti-Affinity in Kubernetes

## üå± Concept Overview

Node Affinity and Anti-Affinity are advanced **scheduling rules** used by Kubernetes to control **which nodes your pods should or should not be scheduled on**, based on node labels.

They allow us to:
- **Affinity**: Attract pods to specific nodes ("prefer to schedule here").
- **Anti-Affinity**: Avoid scheduling pods on the same nodes ("don't schedule near this").

They are similar to `nodeSelector` but more **flexible and expressive**.

---

## üß† Node Affinity
Node Affinity is a rule that tells Kubernetes to place a Pod **on a node that matches a given label**.

### üîß Syntax
Defined under:
```yaml
spec:
  affinity:
    nodeAffinity:
```

### üìå Example: Schedule on a node with label `disktype=ssd`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ssd-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: app
    image: nginx
```

### ‚úçÔ∏è Explanation:
- `requiredDuringSchedulingIgnoredDuringExecution`: This is **mandatory** for the pod to be scheduled. If not matched, the pod will stay in a **Pending** state.
- `disktype=ssd`: The node **must have this label**.

---

## üß† Node Anti-Affinity
Node Anti-Affinity is the opposite. It prevents pods from being scheduled on nodes **that have a specific label**.

### üìå Example: Don't schedule on a node with label `zone=us-east-1a`
```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: NotIn
            values:
            - us-east-1a
```

This is useful to **avoid overloading a zone or group of nodes**.

---

## üß† Soft (Preferred) Affinity
We can also say: "Try to schedule here, but it‚Äôs okay if not" using `preferredDuringSchedulingIgnoredDuringExecution`.

```yaml
preferredDuringSchedulingIgnoredDuringExecution:
  - weight: 1
    preference:
      matchExpressions:
        - key: disktype
          operator: In
          values:
            - ssd
```

This adds **flexibility**, so the pod won't stay pending if no matching node is found.

---

## ü§π‚Äç‚ôÄÔ∏è Real-Life Example
- **Affinity**: You want high-performance workloads to run on SSD-based nodes only.
- **Anti-Affinity**: You want your replicas to be on different zones (to avoid failure impact).

---

## ‚öôÔ∏è Node Labeling Command
To use affinity rules, nodes must be labeled:
```bash
kubectl label nodes node-name disktype=ssd
```

---

## üìö Comparison Table
| Feature                | nodeSelector | Node Affinity                |
|------------------------|--------------|------------------------------|
| Expressiveness         | Low          | High                         |
| Operators supported    | `=`          | `In`, `NotIn`, `Exists`, etc |
| Hard & Soft rules      | ‚ùå           | ‚úÖ                           |

---

## ‚ùì Common Doubts Answered

### ‚úÖ Can I use both `nodeSelector` and `nodeAffinity` together?
Yes. But if both are specified, **both must match**.

### ü§î What happens if nodeAffinity doesn‚Äôt match any node?
Pod will stay in **Pending** state (if using `required...`).

### üìé What is the use of `preferredDuringSchedulingIgnoredDuringExecution`?
It‚Äôs a **soft rule** ‚Äì pod still gets scheduled even if preference not met.

---

## üß≠ Summary
- Use **Node Affinity** to guide pod placement.
- Helps with high availability, performance tuning, fault tolerance.
- It‚Äôs more flexible and powerful than `nodeSelector`.
- Use labels on nodes and match them in pod YAMLs.

---

## ‚úÖ Practice YAMLs
Try creating a pod that:
1. Runs on SSD nodes only.
2. Avoids node labeled `env=dev`.
3. Prefers node in `zone=us-west1-b` but doesn‚Äôt strictly require it.

Let me know if you want those YAMLs added here too!

